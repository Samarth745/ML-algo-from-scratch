{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOpYvN9wrRP9DB3YYc5/kKw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samarth745/ML-algo-from-scratch/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Derivation of Multiple Linear Regression**\n",
        "\n",
        "We now turn our attention to the generalization of linear regression when there are multiple predictor variables. Let us assume that we have $ p $ independent variables $ x_1, x_2, \\dots, x_p $, and a dependent variable $ y $. The multiple linear regression model can be written as\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_p x_p + \\epsilon\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ \\beta_0 $ is the intercept term,\n",
        "- $ \\beta_1, \\beta_2, \\dots, \\beta_p $ are the coefficients corresponding to each independent variable $ x_1, x_2, \\dots, x_p $,\n",
        "- $ \\epsilon $ is the error term, capturing the discrepancy between the observed and predicted values of $ y $.\n",
        "\n",
        "This model can also be written succinctly using vector notation as:\n",
        "\n",
        "$$\n",
        "y = \\mathbf{X} \\boldsymbol{\\beta} + \\epsilon\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $ \\mathbf{X} $ is the $ n \\times (p+1) $ matrix of observations (with a column of ones to account for the intercept),\n",
        "- $ \\boldsymbol{\\beta} = $beta_0, \\beta_1, \\dots, \\beta_p)^T $ is the vector of coefficients, and\n",
        "- $ \\epsilon $ is the vector of error terms.\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "The task is to estimate the vector $ \\boldsymbol{\\beta} $ by minimizing the sum of squared residuals, which is given by:\n",
        "\n",
        "$$\n",
        "S({\\beta}) = \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2\n",
        "$$\n",
        "\n",
        "where $ \\hat{y}_i $ denotes the predicted value of $ y $ based on the model, i.e.,\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\beta_0 + \\beta_1 x_{1i} + \\dots + \\beta_p x_{pi}\n",
        "$$\n",
        "\n",
        "In matrix form, this expression becomes:\n",
        "\n",
        "$$\n",
        "S({\\beta}) = ({y} - {X} {\\beta})^T {y} - {X} {\\beta})\n",
        "$$\n",
        "\n",
        "## **Minimization via Ordinary Least Squares (OLS)**\n",
        "\n",
        "To find the parameter vector $ \\boldsymbol{\\beta} $ that minimizes the residual sum of squares, we take the gradient of $ S$boldsymbol{\\beta} $ with respect to $ \\boldsymbol{\\beta} $ and set it to zero. The gradient is given by:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial S({\\beta})}{\\partial \\boldsymbol{\\beta}} = -2 \\mathbf{X}^T ({y} - \\mathbf{X} \\boldsymbol{\\beta})\n",
        "$$\n",
        "\n",
        "Setting this equal to zero:\n",
        "\n",
        "$$\n",
        "\\mathbf{X}^T ({y} - \\mathbf{X} \\boldsymbol{\\beta}) = 0\n",
        "$$\n",
        "\n",
        "Rearranging, we obtain the **Normal Equation**:\n",
        "\n",
        "$$\n",
        "\\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\beta} = \\mathbf{X}^T \\mathbf{y}\n",
        "$$\n",
        "\n",
        "Assuming $ {X}^T.{X} $ is invertible, the solution to the normal equation is:\n",
        "\n",
        "$$\n",
        "{\\beta} = ({X}^T \\mathbf{X})^{-1} {X}^T.{y}\n",
        "$$\n",
        "\n",
        "This provides the best linear unbiased estimator for $ {\\beta} $, and hence, the regression coefficients are estimated.\n",
        "\n",
        "## **Conclusion**\n",
        "\n",
        "The general form of multiple linear regression allows for the modeling of a linear relationship between a dependent variable and multiple independent variables. The parameter estimates are obtained by minimizing the sum of squared residuals, leading to the normal equation:\n",
        "\n",
        "$$\n",
        "{\\beta} = ({X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
        "$$\n",
        "\n",
        "This expression provides the optimal solution for the regression coefficients under the ordinary least squares criterion.\n"
      ],
      "metadata": {
        "id": "pX1u7tgFZVvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLinearRegression:\n",
        "  import numpy as np\n",
        "  def __init__(self):\n",
        "    ## There are things that we need to find here\n",
        "    ## Weights and Biases\n",
        "    pass\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    num_of_rows, num_of_columns = X.shape ## Getting total rows and columns\n",
        "    b_ = np.ones((num_of_rows, 1)) ## Creating an array of all ones of shape (n,1)\n",
        "    X_b = np.hstack((X,b_)) ## Adding b_ to X to also get the constant parameter in Linear equation\n",
        "    y = y.values.reshape(-1, 1) ## Converting y into (n,1) matric\n",
        "\n",
        "    # Solution of Linear Regression\n",
        "    # $Î’ = (X.X^T)^{-1}.X^T.y$\n",
        "    all_params = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "    all_params = all_params.ravel() # rechanging the dimenstion to (1,n) from (n,1)\n",
        "    constants = all_params[-1]\n",
        "    weights = all_params[:-1]\n",
        "    self.weights = weights\n",
        "    self.constants = constants\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.dot(X, self.weights) + self.constants"
      ],
      "metadata": {
        "id": "c-KjalVDk-h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "myLR = MyLinearRegression()\n",
        "myLR.fit(X_train, y_train)\n",
        "my_pred = myLR.predict(X_test)"
      ],
      "metadata": {
        "id": "HQfYxT0OpfDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "sciki_pred = lr.predict(X_test)"
      ],
      "metadata": {
        "id": "YqxHSFKKq4hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round((my_pred - sciki_pred).mean(), 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHBtm8t9sYPF",
        "outputId": "24f8629a-54c7-40f7-ab75-d500500a34b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    }
  ]
}