{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbdFvNZAQNyRYzHcgRVifl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samarth745/ML-algo-from-scratch/blob/main/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Derivation of Gradient Descent\n",
        "\n",
        "### Objective\n",
        "\n",
        "Given a function $ f(\\mathbf{w}) $, where $ \\mathbf{w} $ represents the parameters (or weights) of the model, the goal of gradient descent is to minimize this function with respect to $ \\mathbf{w} $. The objective function could be, for instance, a loss function such as the Mean Squared Error (MSE) in linear regression.\n",
        "\n",
        "\n",
        "### Gradient\n",
        "The gradient of the function $ f(\\mathbf{w}) $ is a vector of partial derivatives with respect to each parameter\n",
        "\n",
        "$ w_i $. It tells us the direction of the steepest ascent of the function.\n",
        "\n",
        "$\n",
        "\\nabla f(\\mathbf{w}) = \\left[ \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2}, \\dots, \\frac{\\partial f}{\\partial w_n} \\right]\n",
        "$\n",
        "\n",
        "Since we want to minimize $ f(\\mathbf{w}) $, we take steps in the opposite direction of the gradient.\n",
        "\n",
        "### Gradient Descent Update Rule\n",
        "\n",
        "At each iteration $ t $, we update the weights $ \\mathbf{w} $ using the following rule:\n",
        "\n",
        "\\[\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\alpha \\nabla f(\\mathbf{w}^{(t)})\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- $ \\alpha $ is the learning rate, which controls the step size.\n",
        "- $ \\nabla f(\\mathbf{w}^{(t)}) $ is the gradient of the function evaluated at the current parameter values.\n",
        "\n",
        "### Intuition\n",
        "\n",
        "- The term $ \\nabla f(\\mathbf{w}^{(t)}) $ gives the direction and magnitude of the steepest ascent of the function at $ \\mathbf{w}^{(t)} $.\n",
        "- The negative sign ensures that we move in the direction of steepest descent (i.e., toward minimizing $ f $).\n",
        "- The learning rate $ \\alpha $ controls how large the update step is. A small $ \\alpha $ leads to small updates (slow convergence), and a large $ \\alpha $ may lead to overshooting the minimum.\n",
        "\n",
        "### Example: Gradient Descent for Mean Squared Error\n",
        "\n",
        "Suppose $ f(\\mathbf{w}) $ is the Mean Squared Error (MSE) for a linear regression model, defined as:\n",
        "\n",
        "\\[\n",
        "f(\\mathbf{w}) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( y_i - \\mathbf{x}_i^\\top \\mathbf{w} \\right)^2\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- $ m $ is the number of training samples.\n",
        "- $ y_i $ is the true value for sample $ i $.\n",
        "- $ \\mathbf{x}_i $ is the feature vector for sample $ i $.\n",
        "\n",
        "The gradient of the MSE with respect to the weights $ \\mathbf{w} $ is:\n",
        "\n",
        "\\[\n",
        "\\nabla f(\\mathbf{w}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left( y_i - \\mathbf{x}_i^\\top \\mathbf{w} \\right) \\mathbf{x}_i\n",
        "\\]\n",
        "\n",
        "Thus, the update rule for the weights becomes:\n",
        "\n",
        "\\[\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\alpha}{m} \\sum_{i=1}^{m} \\left( y_i - \\mathbf{x}_i^\\top \\mathbf{w}^{(t)} \\right) \\mathbf{x}_i\n",
        "\\]\n",
        "\n",
        "### Convergence\n",
        "\n",
        "By repeating the update rule iteratively, the weights $ \\mathbf{w} $ converge to the values that minimize the objective function $ f(\\mathbf{w}) $, assuming an appropriate choice of the learning rate $ \\alpha $.\n",
        "\n",
        "### Stopping Criteria\n",
        "\n",
        "The algorithm stops when:\n",
        "- The change in $ f(\\mathbf{w}) $ between iterations is smaller than a predefined threshold.\n",
        "- The maximum number of iterations is reached.\n",
        "\n"
      ],
      "metadata": {
        "id": "33SLPpbXhD6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "lHGz5f4-RsX6"
      },
      "outputs": [],
      "source": [
        "# data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_california_housing(return_X_y=True, as_frame=True)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=7)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "KSd3J4aJRzGF"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Batch_Gradient_Descent:\n",
        "  def __init__(self, epochs):\n",
        "    self.epochs = epochs\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    num_of_rows, num_of_columns = X.shape ## Getting total rows and columns\n",
        "    constant = np.random.rand()\n",
        "    weights = np.random.rand(num_of_columns)\n",
        "    alpha = 0.05\n",
        "    for i in range(self.epochs):\n",
        "      y_pred = np.dot(X, weights) + constant ## Calculate the prediction with current weights\n",
        "      residual = y - y_pred ## Calculate the residual\n",
        "\n",
        "      ## calculating the gradient\n",
        "      weight_gradient = (-1/num_of_rows)*np.dot(X.T, residual) ## Derivating wrt weight vectors\n",
        "      constant_gradient = -residual.mean() ## Derivating with respect to constant_vector\n",
        "\n",
        "      ## Update the weights\n",
        "      weights = weights - alpha*weight_gradient\n",
        "      constant = constant - alpha*constant_gradient\n",
        "    self.weights = weights\n",
        "    self.constant = constant\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.dot(X, self.weights) + self.constant"
      ],
      "metadata": {
        "id": "zZY4Y2ZpMyWk"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Stochastic_Gradient_Descent:\n",
        "    def __init__(self, epochs):\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X_, y_):\n",
        "        num_of_rows, num_of_columns = X_.shape  # Getting total rows and columns\n",
        "        constant = np.random.rand()\n",
        "        weights = np.random.rand(num_of_columns)\n",
        "        alpha = 0.09\n",
        "        thetas = []\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            index = np.random.randint(num_of_rows)  # Sample a random row, not column\n",
        "            X = X_[index]\n",
        "            y = y_[index]\n",
        "            y_pred = np.dot(X, weights) + constant  # Calculate the prediction with current weights\n",
        "            residual = y - y_pred  # Calculate the residual\n",
        "\n",
        "            # Calculating the gradient\n",
        "            weight_gradient = -residual * X  # Derivative wrt weight vectors\n",
        "            constant_gradient = -residual  # Derivative with respect to constant\n",
        "\n",
        "            # Update the weights\n",
        "            weights = weights - alpha * weight_gradient\n",
        "            constant = constant - alpha * constant_gradient\n",
        "\n",
        "            # Optionally store the weight values\n",
        "            thetas.append(weights.copy())\n",
        "\n",
        "        self.weights = weights\n",
        "        self.constant = constant\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.constant"
      ],
      "metadata": {
        "id": "4AWjTtcuReYt"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniBatch_Greadient_Descent:\n",
        "  def __init__(self, epochs, batch_size):\n",
        "    self.epochs = epochs\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def fit(self, X_, y_):\n",
        "    num_of_rows, num_of_columns = X_.shape ## Getting total rows and columns\n",
        "    constant = np.random.rand()\n",
        "    weights = np.random.rand(num_of_columns)\n",
        "    alpha = 0.05\n",
        "    for i in range(0,self.epochs):\n",
        "      random_indices = np.random.choice(num_of_rows,\n",
        "                                        size=self.batch_size,\n",
        "                                        replace=False)\n",
        "      X = X_[random_indices]\n",
        "      y=y_[random_indices]\n",
        "      y_pred = np.dot(X, weights) + constant ## Calculate the prediction with current weights\n",
        "      residual = y - y_pred ## Calculate the residual\n",
        "\n",
        "      ## valculating the gradient\n",
        "      weight_gradient = (-1/num_of_rows)*np.dot(X.T, residual) ## Derivating wrt weight vectors\n",
        "      constant_gradient = -residual.mean() ## Derivating with respect to constant_vector\n",
        "\n",
        "      ## Update the weights\n",
        "      weights = weights - alpha*weight_gradient\n",
        "      constant = constant - alpha*constant_gradient\n",
        "    self.weights = weights\n",
        "    self.constant = constant\n",
        "\n",
        "  def predict(self, X):\n",
        "    return np.dot(X, self.weights) + self.constant"
      ],
      "metadata": {
        "id": "cc6ly23hSfnC"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bgd = Batch_Gradient_Descent(epochs=100)\n",
        "bgd.fit(X_train, y_train.values)\n",
        "bgd.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNAIz2SeZRLw",
        "outputId": "15c35741-8136-4a2b-de73-e665c7aa9341"
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.47266656, 2.22112071, 2.35372253, ..., 1.60359203, 2.07906991,\n",
              "       4.13403838])"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = Stochastic_Gradient_Descent(epochs=100)\n",
        "sgd.fit(X_train, y_train.values)\n",
        "sgd.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIEl7U4BPGBM",
        "outputId": "37d6526c-c3d9-4c96-c186-db1bffe326a4"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.10022346, 2.50300633, 2.93163382, ..., 2.07060425, 2.56679668,\n",
              "       4.58261316])"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mgd = MiniBatch_Greadient_Descent(epochs=100, batch_size=20)\n",
        "mgd.fit(X_train, y_train.values)\n",
        "mgd.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAf7LcDmX2bd",
        "outputId": "ee569e41-86dc-49f8-b842-7270d35db93c"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.85326551, 1.82140566, 3.21939156, ..., 3.19734511, 3.2503211 ,\n",
              "       3.48901472])"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6F8wz52YN5q",
        "outputId": "d477a74e-9045-41e2-e1a3-f4ab13aca151"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.65084222, 2.47797721, 2.42550694, ..., 1.86032202, 2.39125575,\n",
              "       3.84598092])"
            ]
          },
          "metadata": {},
          "execution_count": 229
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the values from gradient descent are very similar to Original Values from multiple Linear regression"
      ],
      "metadata": {
        "id": "234pfiKfgc_o"
      }
    }
  ]
}