{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf4aWXJCX91OJwHdVEN5VJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samarth745/ML-algo-from-scratch/blob/main/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression/ Softmax Regression\n",
        "\n",
        "### Softmax Regression\n",
        "\n",
        "In logistic regression, the decision boundary is linear and is represented as:\n",
        "\n",
        "$Z = W_1 X_1 + W_2 X_2 + \\dots + W_n X_n + C$\n",
        "\n",
        "Where:\n",
        "- $W_1, W_2, \\dots, W_n$ are the weights associated with the features $X_1, X_2, \\dots, X_n$,\n",
        "- $C$ is the bias term,\n",
        "- $Z$ is the linear combination of weights and inputs.\n",
        "\n",
        "### Adjusting the Decision Boundary\n",
        "\n",
        "To improve the model, we need the decision boundary to \"push\" the line when points are correctly classified and \"pull\" or \"push\" the boundary based on the distance of points. A step function can achieve a classification, but itâ€™s not differentiable, making optimization difficult.\n",
        "\n",
        "Instead of using a **Step Function** to classify points, we use the **Sigmoid Function** to satisfy the two key conditions:\n",
        "- Push the decision boundary when points are correctly classified.\n",
        "- Adjust the boundary based on the distance from the decision boundary.\n",
        "\n",
        "The **Sigmoid Function** is given by:\n",
        "\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
        "\n",
        "This maps the output to a probability value between 0 and 1, which is useful for binary classification problems.\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations of Using the Step Function Directly\n",
        "\n",
        "While the sigmoid function offers a solution, iterating over the points and asking \"which region does it belong to?\" shifts the decision boundary but doesn't ensure a well-generalized solution. This is because:\n",
        "- Manually shifting the boundary isn't efficient.\n",
        "- This method does not capture the \"best\" line; a loss function is required for optimization.\n",
        "\n",
        "### Using the Logistic Function for Prediction\n",
        "\n",
        "For each point $i$, the predicted probability of being in a particular class (e.g., class 1) is given by:\n",
        "\n",
        "$$\\hat{y}_i = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-(W_1 x_1 + W_2 x_2 + \\dots + W_n x_n + C)}}$$\n",
        "\n",
        "Where $\\hat{y}_i$ is the predicted probability for point $i$.\n",
        "\n",
        "This can be interpreted as:\n",
        "- $\\hat{y}_i$ is the probability that point $i$ belongs to a particular class (e.g., green or red).\n",
        "- For binary classification:\n",
        "  - $\\hat{y}_i = P(G)$, the probability that the point is green.\n",
        "  - $1 - \\hat{y}_i = P(R)$, the probability that the point is red.\n",
        "\n",
        "---\n",
        "\n",
        "### Loss Function: Maximum Likelihood Estimation\n",
        "\n",
        "To measure the performance of the logistic regression model, we aim to maximize the probability of observing the true labels given the predicted probabilities. This is called **Maximum Likelihood Estimation (MLE)**. In the case of logistic regression, the likelihood function is:\n",
        "\n",
        "$$L = P(y_1) \\cdot P(y_2) \\cdot P(y_3) \\dots P(y_N)$$\n",
        "\n",
        "Where $P(y_i)$ is the probability assigned to the correct class for point $i$.\n",
        "\n",
        "However, multiplying these probabilities often leads to very small values, which can cause numerical instability. Instead, we maximize the logarithm of the likelihood function, leading to **Log-Loss** (or **Binary Cross-Entropy**).\n",
        "\n",
        "---\n",
        "\n",
        "### Binary Cross-Entropy (Log-Loss) Function\n",
        "\n",
        "The **Log-Loss Function** is the negative log likelihood, given by:\n",
        "\n",
        "$$H(p, q) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$$\n",
        "\n",
        "Where:\n",
        "- $y_i$ is the true label of point $i$ (either 0 or 1),\n",
        "- $\\hat{y}_i$ is the predicted probability for point $i$,\n",
        "- $N$ is the total number of data points.\n",
        "\n",
        "The goal of logistic regression is to minimize this **cross-entropy loss** function, which ensures that predicted probabilities $\\hat{y}_i$ are as close as possible to the true labels $y_i$.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary\n",
        "\n",
        "- **Logistic Regression** uses the sigmoid function to model the probability that a given input belongs to a particular class.\n",
        "- The model aims to maximize the likelihood of the true labels by minimizing the **binary cross-entropy** loss function.\n",
        "- This approach ensures that the decision boundary is adjusted in a way that maximizes classification performance.\n"
      ],
      "metadata": {
        "id": "bgK3vrLtIxi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LogisticRegression:\n",
        "  def __init__(self, alpha=0.5, epochs = 500, threshold = 1e-5):\n",
        "    self.alpha = alpha\n",
        "    self.epochs = epochs\n",
        "    self.threshold = threshold\n",
        "\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    num_of_classes = len(np.unique(y))\n",
        "    num_of_columns = X.shape[1]\n",
        "    num_of_rows = X.shape[0]\n",
        "    self.constants = np.random.rand(num_of_classes)\n",
        "    self.weights = np.random.rand(num_of_classes,num_of_columns)\n",
        "    Y = np.diag(np.ones(num_of_classes))[y] ## Create diagonal matrix with 1 and multiply with y to get one hot encoding\n",
        "    prev_J = np.inf\n",
        "    for iter in range(self.epochs):\n",
        "      z = (X @ self.weights.T) + self.constants ## Calculate X\n",
        "      sig_out = np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True) ## Sigmoid value for Z\n",
        "      errors = Y - sig_out ## Total Error\n",
        "      J = -((Y * np.log(sig_out)).sum(axis=1)).mean() ## Cost Function\n",
        "      if np.abs(prev_J - J) < self.threshold:\n",
        "        print(f\"Algorithm completed at {iter} Iterations\")\n",
        "        break\n",
        "      else:\n",
        "        ## Calculating Gradient\n",
        "        weight_gradient = (-1 /num_of_rows) * (errors.T @ X)\n",
        "        constant_gradient = -errors.mean(axis=0)\n",
        "\n",
        "        ## Update self.weights and Bias\n",
        "        self.weights = self.weights - (self.alpha * weight_gradient)\n",
        "        self.constants = self.constants - (self.alpha * constant_gradient)\n",
        "        prev_J = J\n",
        "\n",
        "  def predict(self, X):\n",
        "    z = (X @ self.weights.T) + constants ## Calculate X\n",
        "    sig_out = np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True) ## Sigmoid\n",
        "    return np.argmax(sig_out, axis=1)\n",
        "\n",
        "\n",
        "  def predict_proba(self, X):\n",
        "    z = (X @ self.weights.T) + constants ## Calculate X\n",
        "    sig_out = np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True) ## Sigmoid\n",
        "    return np.round(sig_out, decimals=2)\n"
      ],
      "metadata": {
        "id": "ZvckNITgk6Dp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,\n",
        "                                                    y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    stratify=y,\n",
        "                                                    random_state=7)"
      ],
      "metadata": {
        "id": "B3_tB44hMjvh"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.predict(X_test)"
      ],
      "metadata": {
        "id": "8kixE0ohMnGH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b35bc0-990d-4b3a-9656-215ec8f4b88b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 0, 1, 2, 1, 2, 0, 2, 2, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 0,\n",
              "       1, 0, 2, 2, 1, 1, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "lr.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiPPsjOclMC5",
        "outputId": "ef2e36d4-db0e-41cc-8fa0-a1373c2f89e6"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 0, 1, 2, 1, 2, 0, 2, 2, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 0,\n",
              "       1, 0, 2, 2, 2, 1, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}